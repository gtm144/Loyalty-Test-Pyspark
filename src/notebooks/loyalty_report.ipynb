{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cce4174",
   "metadata": {},
   "source": [
    "# Loyalty Market Test\n",
    "\n",
    "Data is stored in a S3 bucket, I need to fetch it using a hadoop library and then start testing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12d174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "s3_access_key = os.environ.get(\"S3_ACCESS_KEY\")\n",
    "s3_secret_key = os.environ.get(\"S3_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b365847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Remember to use 2 cores for laptop work and 4 cores for local machine\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Loyalty\") \\\n",
    "    .config(\"spark.master\", \"local[4]\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.3.1\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ed4aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----------+---------+----------------+----------+--------+\n",
      "|product_key|sku|       upc|item_name|item_description|department|category|\n",
      "+-----------+---+----------+---------+----------------+----------+--------+\n",
      "| 7652613339|  0|7652613339|   324168|              NA|  651b1068|8312aed6|\n",
      "| 1810063322|  0|1810063322|   276973|              NA|  651b1068|7aaa7a34|\n",
      "| 5274585486|  0|5274585486|   794396|              NA|  c81ba571|54ea8364|\n",
      "| 6978362094|  0|6978362094|   510386|              NA|  b947a4a9| 382cf3a|\n",
      "| 6978396053|  0|6978396053|   120105|              NA|  b947a4a9| 382cf3a|\n",
      "+-----------+---+----------+---------+----------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(\"s3a://data-test-202302/product.csv\", header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c14eac",
   "metadata": {},
   "source": [
    "## Understand work around partitions\n",
    "To use the most of cores, partitions must be in multiples of number of cores in order to take the most advantage of processing power. \n",
    "\n",
    "**- On Laptop** The number of partitions processs by a core would be 2.\n",
    "\n",
    "**- On Local machine** The number of partitions processs by a core would be just one.\n",
    "\n",
    "As its expected, on laptop it would take more time to process data\n",
    "\n",
    "TODO Show processing times and analyse spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3067f6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions: \",df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0491b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: 1155399 bytes\n",
      "Partition 1: 1156855 bytes\n",
      "Partition 2: 1155973 bytes\n",
      "Partition 3: 720132 bytes\n"
     ]
    }
   ],
   "source": [
    "partition_sizes = df.rdd.mapPartitions(lambda partition: [sum(map(len, partition))]).collect()\n",
    "for i, size in enumerate(partition_sizes):\n",
    "    print(f\"Partition {i}: {size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a77d16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
